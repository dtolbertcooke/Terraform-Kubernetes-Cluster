# This workflow destroys the Terraform cloud infrastructure.
# Steps:
# 1. Fetch backend config from SSM
# 2. Initialize Terraform with remote backend
# 3. Destroy resources
name: Destroy Infrastructure Pipeline
description: Destroys Terraform cloud infrastructure.

on:
  workflow_dispatch: # Manual trigger so you only run destroy when needed
    inputs:
      region:
        description: "Region to destroy resources in (i.e. us-east-1, eu-west-2, etc.)"
        type: string
        required: true

permissions:
  id-token: write # Required for github oidc authentication with AWS
  contents: read # Allow repository contents to be checked out

jobs:
  destroy:
    runs-on: ubuntu-latest

    defaults:
      run:
        working-directory: infra # Keep runs scoped to terraform code

    steps:
      # checkout the repo
      - name: Checkout repo
        uses: actions/checkout@v4

      # configure aws credentials using github environment secrets
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ github.event.inputs.region }}
          role-to-assume: arn:aws:iam::${{ vars.AWS_ACCOUNT_ID }}:role/github-oidc-role-dev

      # install terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.12.2" # Pin to specific version

      # init terraform locally on github runner
      - name: Terraform Init 
        run: terraform init

      # Export Terraform outputs to get cluster name
      - name: Export Terraform Outputs
        run: terraform output -json > tf_outputs.json

      # Install jq for parsing JSON
      - name: Install jq
        run: sudo apt-get install -y jq

      # Get cluster name from outputs
      - name: Get Cluster Name
        id: cluster-check
        run: |
          CLUSTER_NAME=$(jq -r '.cluster_name.value // empty' tf_outputs.json)
          if [ -z "$CLUSTER_NAME" ] || [ "$CLUSTER_NAME" = "null" ]; then
            echo "No EKS cluster found in state, skipping Kubernetes cleanup"
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
          else
            # Check cluster status
            CLUSTER_STATUS=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "${{ github.event.inputs.region }}" --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
            
            if [ "$CLUSTER_STATUS" = "ACTIVE" ]; then
              echo "EKS_CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV
              echo "cluster_exists=true" >> $GITHUB_OUTPUT
              echo "Cluster is ACTIVE, will clean up Kubernetes resources"
            else
              echo "Cluster status is $CLUSTER_STATUS, skipping Kubernetes cleanup"
              echo "cluster_exists=false" >> $GITHUB_OUTPUT
            fi
          fi

      # Setup kubectl to interact with cluster
      - name: Setup kubectl
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        uses: azure/setup-kubectl@v4

      # Configure kubeconfig for EKS cluster
      - name: Configure kubeconfig
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          aws eks update-kubeconfig \
            --name "${{ env.EKS_CLUSTER_NAME }}" \
            --region "${{ github.event.inputs.region }}"

      # Setup Helm
      - name: Setup Helm
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        uses: azure/setup-helm@v4

      # Delete Helm deployments (removes LoadBalancers)
      - name: Delete Helm Deployments
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          helm list --all-namespaces -o json | jq -r '.[] | "\(.name) -n \(.namespace)"' | while read release; do
            echo "Uninstalling $release"
            helm uninstall $release || true
          done
        continue-on-error: true

      # Wait for LoadBalancers and ENIs to be deleted
      - name: Wait for AWS Resources Cleanup
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          echo "Waiting for AWS to clean up LoadBalancers and network interfaces..."
          
          # Poll for LoadBalancer services to be fully deleted
          for i in {1..20}; do
            LB_COUNT=$(kubectl get svc --all-namespaces -o json | jq '[.items[] | select(.spec.type=="LoadBalancer")] | length')
            if [ "$LB_COUNT" -eq 0 ]; then
              echo "All LoadBalancer services deleted"
              break
            fi
            echo "Attempt $i/20: $LB_COUNT LoadBalancer(s) still present, waiting 30s..."
            sleep 30
          done
          
          # Additional wait for AWS to release ENIs
          echo "Waiting additional 120 seconds for ENI cleanup..."
          sleep 120

      # Verify no services remain
      - name: Verify Services Deleted
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: kubectl get svc --all-namespaces
        continue-on-error: true

      # destroy config
      - name: Terraform Destroy
        run: terraform destroy -auto-approve -lock=false 
