# This workflow destroys the Terraform cloud infrastructure.
# Steps:
# 1. Fetch backend config from SSM
# 2. Initialize Terraform with remote backend
# 3. Destroy resources
name: Destroy Infrastructure Pipeline
description: Destroys Terraform cloud infrastructure.

on:
  workflow_dispatch: # Manual trigger so you only run destroy when needed
    inputs:
      region:
        description: "Region to destroy resources in (i.e. us-east-1, eu-west-2, etc.)"
        type: string
        required: true

permissions:
  id-token: write # Required for github oidc authentication with AWS
  contents: read # Allow repository contents to be checked out

jobs:
  destroy:
    runs-on: ubuntu-latest

    defaults:
      run:
        working-directory: infra # Keep runs scoped to terraform code

    steps:
      # checkout the repo
      - name: Checkout repo
        uses: actions/checkout@v4

      # configure aws credentials using github environment secrets
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ vars.AWS_ACCOUNT_ID }}:role/github-oidc-role-dev 
          aws-region: ${{ github.event.inputs.region }}

      - name: Fetch Backend Config from SSM
        run: |
          STATE_BUCKET=$(aws ssm get-parameter --name "/tf/global-backend/state-bucket" --query "Parameter.Value" --output text --region ${{ github.event.inputs.region }})
          STATE_TABLE=$(aws ssm get-parameter --name "/tf/global-backend/state-table" --query "Parameter.Value" --output text --region ${{ github.event.inputs.region }})
          REGION=$(aws ssm get-parameter --name "/tf/global-backend/region" --query "Parameter.Value" --output text --region ${{ github.event.inputs.region }})

          echo "STATE_BUCKET=$STATE_BUCKET" >> $GITHUB_ENV
          echo "STATE_TABLE=$STATE_TABLE" >> $GITHUB_ENV
          echo "REGION=$REGION" >> $GITHUB_ENV

          cat > eks.hcl <<EOF
          bucket         = "$STATE_BUCKET"
          key            = "eks-project/terraform.tfstate"
          region         = "$REGION"
          dynamodb_table = "$STATE_TABLE"
          encrypt        = true
          EOF

      # install terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.12.2" # Pin to specific version

      - name: Clean stale Terraform lock
        run: |
          LOCK_KEY="${STATE_BUCKET}/eks-project/terraform.tfstate"
          aws dynamodb delete-item \
            --table-name "$STATE_TABLE" \
            --key "{\"LockID\": {\"S\": \"$LOCK_KEY\"}}" \
            --region ${{ github.event.inputs.region }} || true

      # init terraform locally on github runner
      - name: Terraform Init 
        run: terraform init -backend-config=eks.hcl

      # Export Terraform outputs to get cluster name
      - name: Export Terraform Outputs
        run: terraform output -json > tf_outputs.json

      # Install jq for parsing JSON
      - name: Install jq
        run: sudo apt-get install -y jq

      # Get cluster name from outputs
      - name: Get Cluster Name
        id: cluster-check
        run: |
          CLUSTER_NAME=$(jq -r '.cluster_name.value // empty' tf_outputs.json)

          if [ -n "$CLUSTER_NAME" ] && [ "$CLUSTER_NAME" != "null" ]; then
            echo "EKS_CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
          else
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
          fi

      # Setup kubectl to interact with cluster
      - name: Setup kubectl
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        uses: azure/setup-kubectl@v4

      # Configure kubeconfig for EKS cluster
      - name: Configure kubeconfig
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          aws eks update-kubeconfig \
            --name "${{ env.EKS_CLUSTER_NAME }}" \
            --region "${{ github.event.inputs.region }}"

      # Setup Helm
      - name: Setup Helm
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        uses: azure/setup-helm@v4

      # Delete Helm deployments (removes LoadBalancers)
      - name: Uninstall Helm Releases
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          RELEASES=$(helm list --all-namespaces -o json | jq -r '.[] | "\(.name) -n \(.namespace)"')

          if [ -z "$RELEASES" ]; then
            echo "No Helm releases found."
          else
            echo "$RELEASES" | while read release; do
              echo "Uninstalling $release"
              helm uninstall $release || true
            done
          fi

      # Wait for LoadBalancers and ENIs to be deleted
      - name: Wait for LoadBalancer deletion
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          echo "Waiting for LoadBalancer ENIs to be removed..."

          # If cluster API is already gone, skip safely
          if ! kubectl get nodes >/dev/null 2>&1; then
            echo "Cluster API is unreachable. Assuming LoadBalancers are already deleted."
            exit 0
          fi

          # Normal wait loop
          for i in {1..20}; do
            # If kubectl fails at any time, assume cleanup done
            if ! LB_COUNT=$(kubectl get svc --all-namespaces -o json 2>/dev/null | jq '[.items[] | select(.spec.type=="LoadBalancer")] | length'); then
              echo "kubectl no longer has access to the cluster. Assuming LoadBalancers are gone."
              break
            fi

            if [ "$LB_COUNT" -eq 0 ]; then
              echo "All LoadBalancer services deleted."
              break
            fi

            echo "Attempt $i/20: $LB_COUNT still remain... waiting 30s..."
            sleep 30
          done

      # destroy config
      - name: Terraform Destroy
        run: |
          terraform destroy -auto-approve \
          -var="aws_account_id=${{ vars.AWS_ACCOUNT_ID }}" \
          -var="admin_iam_user=Doug" \
          -var="cluster_name=test_cluster_name" \
          -var="project_name=test_project_name"