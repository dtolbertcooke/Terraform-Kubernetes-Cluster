# This workflow destroys the Terraform cloud infrastructure.
# Steps:
# 1. Fetch backend config from SSM
# 2. Initialize Terraform with remote backend
# 3. Destroy resources
name: Destroy Infrastructure Pipeline
description: Destroys Terraform cloud infrastructure.

on:
  workflow_dispatch: # Manual trigger so you only run destroy when needed
    inputs:
      region:
        description: "Region to destroy resources in (i.e. us-east-1, eu-west-2, etc.)"
        type: string
        required: true

permissions:
  id-token: write # Required for github oidc authentication with AWS
  contents: read # Allow repository contents to be checked out

jobs:
  destroy:
    runs-on: ubuntu-latest

    defaults:
      run:
        working-directory: infra # Keep runs scoped to terraform code

    steps:
      # checkout the repo
      - name: Checkout repo
        uses: actions/checkout@v4

      # configure aws credentials using github environment secrets
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ github.event.inputs.region }}

      # Fetch backend config from SSM
      # - Retrieves global S3/DynamoDB state backend
      # - Creates backend config file for terraform init
      - name: Fetch Backend Config from SSM
        run: |
          STATE_BUCKET=$(aws ssm get-parameter --name "/tf/global-backend/state-bucket" --query "Parameter.Value" --output text --region ${{ github.event.inputs.region }})
          STATE_TABLE=$(aws ssm get-parameter --name "/tf/global-backend/state-table" --query "Parameter.Value" --output text --region ${{ github.event.inputs.region }})
          REGION=$(aws ssm get-parameter --name "/tf/global-backend/region" --query "Parameter.Value" --output text --region ${{ github.event.inputs.region }})

          echo "STATE_BUCKET=$STATE_BUCKET" >> $GITHUB_ENV
          echo "STATE_TABLE=$STATE_TABLE" >> $GITHUB_ENV
          echo "REGION=$REGION" >> $GITHUB_ENV

          echo "TF_VAR_state_bucket_name=$STATE_BUCKET" >> $GITHUB_ENV

          cat > eks.hcl <<EOF
          bucket         = "$STATE_BUCKET"
          key            = "eks-project/terraform.tfstate"
          region         = "$REGION"
          dynamodb_table = "$STATE_TABLE"
          encrypt        = true
          EOF

      # install terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.12.2" # Pin to specific version

      # init terraform locally on github runner
      - name: Terraform Init 
        run: terraform init -backend-config=eks.hcl

      # Export Terraform outputs to get cluster name
      - name: Export Terraform Outputs
        run: terraform output -json > tf_outputs.json

      # Install jq for parsing JSON
      - name: Install jq
        run: sudo apt-get install -y jq

      # Get cluster name from outputs
      - name: Get Cluster Name
        id: cluster-check
        run: |
          CLUSTER_NAME=$(jq -r '.cluster_name.value // empty' tf_outputs.json)
          if [ -z "$CLUSTER_NAME" ] || [ "$CLUSTER_NAME" = "null" ]; then
            echo "No EKS cluster found in state, skipping Kubernetes cleanup"
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
          else
            echo "EKS_CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
          fi

      # Setup kubectl to interact with cluster
      - name: Setup kubectl
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        uses: azure/setup-kubectl@v4

      # Configure kubeconfig for EKS cluster
      - name: Configure kubeconfig
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          aws eks update-kubeconfig \
            --name "${{ env.EKS_CLUSTER_NAME }}" \
            --region "${{ github.event.inputs.region }}"

      # Setup Helm
      - name: Setup Helm
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        uses: azure/setup-helm@v4

      # Delete Helm deployments (removes LoadBalancers)
      - name: Delete Helm Deployments
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          helm list --all-namespaces -o json | jq -r '.[] | "\(.name) -n \(.namespace)"' | while read release; do
            echo "Uninstalling $release"
            helm uninstall $release || true
          done
        continue-on-error: true

      # Wait for LoadBalancers to be deleted
      - name: Wait for AWS Resources Cleanup
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          echo "Waiting 120 seconds for AWS to clean up LoadBalancers..."
          sleep 120

      # Verify no services remain
      - name: Verify Services Deleted
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: kubectl get svc --all-namespaces
        continue-on-error: true

      # destroy config
      - name: Terraform Destroy
        run: terraform destroy -auto-approve -lock=false 
