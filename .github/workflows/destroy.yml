# This workflow destroys the Terraform cloud infrastructure.
# Steps:
# 1. Fetch backend config from SSM
# 2. Initialize Terraform with remote backend
# 3. Destroy resources
name: Destroy Infrastructure Pipeline
description: Destroys Terraform cloud infrastructure.

on:
  workflow_dispatch: # Manual trigger so you only run destroy when needed
    inputs:
      region:
        description: "Region to destroy resources in (i.e. us-east-1, eu-west-2, etc.)"
        type: string
        required: true

permissions:
  id-token: write # Required for github oidc authentication with AWS
  contents: read # Allow repository contents to be checked out

jobs:
  destroy:
    runs-on: ubuntu-latest

    defaults:
      run:
        working-directory: infra # Keep runs scoped to terraform code

    steps:
      # checkout the repo
      - name: Checkout repo
        uses: actions/checkout@v4

      # configure aws credentials using github environment secrets
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ github.event.inputs.region }}

      # install terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.12.2" # Pin to specific version

      - name: Clean stale Terraform lock
        run: |
          LOCK_KEY="state-bucket-global-43319381e1/eks-project/terraform.tfstate"
          aws dynamodb delete-item \
            --table-name "state-table-global-infra" \
            --key "{\"LockID\": {\"S\": \"$LOCK_KEY\"}}" || true

      # init terraform locally on github runner
      - name: Terraform Init 
        run: terraform init

      # Export Terraform outputs to get cluster name
      - name: Export Terraform Outputs
        run: terraform output -json > tf_outputs.json

      # Install jq for parsing JSON
      - name: Install jq
        run: sudo apt-get install -y jq

      # Get cluster name from outputs
      - name: Get Cluster Name
        id: cluster-check
        run: |
          CLUSTER_NAME=$(jq -r '.cluster_name.value // empty' tf_outputs.json)

          if [ -n "$CLUSTER_NAME" ] && [ "$CLUSTER_NAME" != "null" ]; then
            echo "EKS_CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
          else
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
          fi

      # Setup kubectl to interact with cluster
      - name: Setup kubectl
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        uses: azure/setup-kubectl@v4

      # Configure kubeconfig for EKS cluster
      - name: Configure kubeconfig
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          aws eks update-kubeconfig \
            --name "${{ env.EKS_CLUSTER_NAME }}" \
            --region "${{ github.event.inputs.region }}"

      # Setup Helm
      - name: Setup Helm
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        uses: azure/setup-helm@v4

      # Delete Helm deployments (removes LoadBalancers)
      - name: Uninstall Helm Releases
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          RELEASES=$(helm list --all-namespaces -o json | jq -r '.[] | "\(.name) -n \(.namespace)"')

          if [ -z "$RELEASES" ]; then
            echo "No Helm releases found."
          else
            echo "$RELEASES" | while read release; do
              echo "Uninstalling $release"
              helm uninstall $release || true
            done
          fi

      # Wait for LoadBalancers and ENIs to be deleted
      - name: Wait for LoadBalancer deletion
        if: steps.cluster-check.outputs.cluster_exists == 'true'
        run: |
          echo "Waiting for LoadBalancer ENIs to be removed..."
          for i in {1..20}; do
            LB_COUNT=$(kubectl get svc --all-namespaces -o json | jq '[.items[] | select(.spec.type=="LoadBalancer")] | length')
            if [ "$LB_COUNT" -eq 0 ]; then
              echo "All LoadBalancer services deleted."
              break
            fi
            echo "Attempt $i/20: $LB_COUNT still remain... waiting 30s..."
            sleep 30
          done

      # destroy config
      - name: Terraform Destroy
        run: terraform destroy -auto-approve
